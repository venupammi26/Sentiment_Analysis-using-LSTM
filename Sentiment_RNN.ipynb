{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z8HL3O34U6o",
        "colab_type": "text"
      },
      "source": [
        "**Load in and visualize the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6fMInIf4KBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIXwlfwS47Q7",
        "colab_type": "text"
      },
      "source": [
        "Set the vocabulary size and load in training and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDXhKk1j4i6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ae7303c3-c159-4526-a447-24e9a81db4f7"
      },
      "source": [
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS2jI_vv4_-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "91d607e2-147f-4aee-969d-453ac3e407d7"
      },
      "source": [
        "print('---review---')\n",
        "print(X_train[6])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---review---\n",
            "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQcl9LBT6L_k",
        "colab_type": "text"
      },
      "source": [
        "Note that the review is stored as a sequence of integers. These are word IDs that have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive).\n",
        "\n",
        "We can use the dictionary returned by imdb.get_word_index() to map the review back to the original words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7nKAIWH5VRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b3fcda97-d12d-4d41-dae1-657c9050181c"
      },
      "source": [
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[6]])\n",
        "print('---label---')\n",
        "print(y_train[6])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n",
            "---review with words---\n",
            "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
            "---label---\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orIRYduu6emq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "36d7d775-24e8-4741-c6f3-dd94b9def2d1"
      },
      "source": [
        "#Maximum review length and minimum review length.\n",
        "print('Maximum review length: {}'.format(\n",
        "len(max((X_train + X_test), key=len))))\n",
        "\n",
        "print('Minimum review length: {}'.format(\n",
        "len(min((X_train + X_test), key=len))))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 2697\n",
            "Minimum review length: 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiOZxARg7cpJ",
        "colab_type": "text"
      },
      "source": [
        "**Pad sequences**\n",
        "In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value (0). We can accomplish this using the pad_sequences() function in Keras. For now, set max_words to 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_W-v8Q27CFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "max_words = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbx9XD-i7sio",
        "colab_type": "text"
      },
      "source": [
        "**Design an RNN model for sentiment analysis**\n",
        "We start building our model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.\n",
        "Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_words, and our output is a binary sentiment label (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExoWitvm7jnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "08905807-038c-4ba9-ba57-d1010faaa248"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "embedding_size=32\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-zDylyG8Tz4",
        "colab_type": "text"
      },
      "source": [
        "To summarize, our model is a simple RNN model with 1 embedding, 1 LSTM and 1 dense layers. 213,301 parameters in total need to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81NjqLfV8bUf",
        "colab_type": "text"
      },
      "source": [
        "**Train and evaluate our model**\n",
        "We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EePWy0lU8UwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_u12v58kAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "69ffd5c1-425e-4680-c499-62161cd1b9d7"
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "hist=model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 24936 samples, validate on 64 samples\n",
            "Epoch 1/3\n",
            "24936/24936 [==============================] - 485s 19ms/step - loss: 0.2043 - accuracy: 0.9221 - val_loss: 0.2340 - val_accuracy: 0.8906\n",
            "Epoch 2/3\n",
            "24936/24936 [==============================] - 481s 19ms/step - loss: 0.1772 - accuracy: 0.9339 - val_loss: 0.1718 - val_accuracy: 0.9062\n",
            "Epoch 3/3\n",
            "24936/24936 [==============================] - 482s 19ms/step - loss: 0.1495 - accuracy: 0.9452 - val_loss: 0.2060 - val_accuracy: 0.9219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI4rFOl-82Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "84783fdd-3588-4e97-8c34-6865080bf173"
      },
      "source": [
        "model.evaluate(X_train,y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 66s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18910219787597657, 0.932479977607727]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJuYeP3aKnaB",
        "colab_type": "text"
      },
      "source": [
        "Once we have trained our model, it’s time to see how well it performs on unseen test data.\n",
        "scores[1] will correspond to accuracy if we pass metrics=[‘accuracy’]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abOTQBM5J3LW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91b4752b-1dce-4f12-8007-76616052cbba"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8745599985122681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSaSJiKK_R2",
        "colab_type": "text"
      },
      "source": [
        "There are several ways in which we can build our model. We can continue trying and improving the accuracy of our model by experimenting with different architectures, layers and parameters."
      ]
    }
  ]
}